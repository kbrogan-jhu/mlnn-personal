{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "Find your favorite news source and grab the article text. \n",
    "\n",
    "1. Show the most common words in the article.\n",
    "2. Show the most common words under a part of speech. (i.e. NOUN: {'Bob':12, 'Alice':4,})\n",
    "3. Find a subject/object relationship through the dependency parser in any sentence.\n",
    "4. Show the most common Entities and their types. \n",
    "5. Find Entites and their dependency (hint: entity.root.head)\n",
    "6. Find the most similar words in the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note: Yes, the notebook from the video is not provided, I leave it to you to make your own :) it's your final assignment for the semester. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install spacy\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'> Reading in data source</font>\n",
    "<font color='purple'> Data pulled from Small Business Admin. site: https://www.sbir.gov/sbirsearch/award/all/?topic=AI&f%5B0%5D=im_field_agencies%3A105738</font>\n",
    "<font color='purple'>All 78 of these awards were made to small bussiness, via the National Science Foundation's SBIR/STTR award program. All of these awards were tagged as AI-related.</font>\n",
    "<font color='purple'> I would like to use Spacy to explore the abstract data for all 78 awards. The goal is to just generate some keywords, specific to AI awards. Eventually, I want to find a way to generate keywords that can be used to search for AI-related awards, given the abstract text. I believe that AI-related tools are likely used in many awarded projects that are tagged under other topic areas. The goal is to find a way to pull those AI-related awards using a list of keywords.</font>\n",
    "\n",
    "#### <font color='purple'> I am narrowing this data down so that only a list containing the abstract text for each proposal is left.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "award_data = pd.read_excel(\"NSF_AI_sbirAwards.xlsx\")\n",
    "abstracts = award_data[\"Abstract\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'> Iterating through each abstract in the list and running the abstract through the NLP</font>\n",
    "\n",
    "<font color='purple'>Since there were 78 awards in my data, this will output a list of 78 doc objects (78 separately processed text blocks in a list). </font>\n",
    "\n",
    "<font color='purple'> I did not want to concatenate all of the abstract text together into one string because I wanted to run each abstract through the NLP as it's own entity. I figured that may be a better way to preserve the integrity of any vector relationships that only exist in a small number of abstracts.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_processed = []\n",
    "\n",
    "for i in abstracts.index:\n",
    "    abstracts_processed.append(nlp(abstracts[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>1. Finding the most common words</font>\n",
    "\n",
    "<font color='purple'> I will first iterate through each abstract and extract the 10 most common words in each abstract. I will keep a running list of each abstract's most common words. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = []\n",
    "\n",
    "for i in range(len(abstracts_processed)):\n",
    "    words = [token.lemma_ for token in abstracts_processed[i] \n",
    "             if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    common10 = Counter(words).most_common(10)\n",
    "    most_common_words.append(common10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'>Since I want the most common words overall (accross all abstracts), I need to iterate through the 10 most common words in each of the 78 abstracts. I will extract only the words themselves (I will not extract the count) and add them to a running list. Having all the most common words from all abstracts in a single list will allow me to see the most common words across all abstracts, rather than just from one abstract at a time. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words_long = []\n",
    "\n",
    "for i in range(len(most_common_words)):\n",
    "    for a in range(len(most_common_words[i])):\n",
    "        most_common_words_long.append(most_common_words[i][a][0])\n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'> Finally, to find the most common words across all 78 abstracts, I will find the most common words in the full list (most_common_words_long). </font>\n",
    "\n",
    "<font color='purple'>In the code, I am also removing certain words from this list, since they fall into the boiler plate language (template_txt) used in all of the abstracts.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "template_txt = ['project', 'broader', 'impact', 'Small', 'Business', 'Innovation', \n",
    "                '\\n', 'AI', 'SBIR','STTR', 'Phase', 'Research', 'propose', 'broad']\n",
    "\n",
    "for i in range(len(template_txt)):\n",
    "    remove_word = template_txt[i]\n",
    "    while remove_word in most_common_words_long:\n",
    "        most_common_words_long.remove(remove_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model', 10),\n",
       " ('learning', 9),\n",
       " ('system', 9),\n",
       " ('datum', 9),\n",
       " ('result', 8),\n",
       " ('technology', 7),\n",
       " ('time', 6),\n",
       " ('provide', 6),\n",
       " ('platform', 6),\n",
       " ('reduce', 5),\n",
       " ('learn', 5),\n",
       " ('health', 5),\n",
       " ('language', 4),\n",
       " ('control', 4),\n",
       " ('increase', 4),\n",
       " ('student', 4),\n",
       " ('video', 4),\n",
       " ('commercial', 4),\n",
       " ('enable', 4),\n",
       " ('information', 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(most_common_words_long).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>2. Finding the most common nouns, adjectives, and verbs.</font>\n",
    "\n",
    "<font color='purple'> For this iteration, I don't care as much about granularity, so I will not treat each abstract as its own entity. I am going to iterate through every abstract and pick out all verbs, nouns, and adjectives. I will then narrow each list down to the most common of each. </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nouns = []\n",
    "all_adj = []\n",
    "all_verbs = []\n",
    "\n",
    "\n",
    "for i in range(len(abstracts_processed)):\n",
    "    nouns = [token.lemma_ for token in abstracts_processed[i] \n",
    "             if token.pos_ == \"NOUN\"]\n",
    "    adj = [token.lemma_ for token in abstracts_processed[i]\n",
    "              if token.pos_ == \"ADJ\"]\n",
    "    verbs = [token.lemma_ for token in abstracts_processed[i]\n",
    "                 if token.pos_ == \"VERB\"]\n",
    "    \n",
    "    all_nouns.append(nouns)\n",
    "    all_adj.append(adj)\n",
    "    all_verbs.append(verbs)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nouns = list(np.concatenate(all_nouns))\n",
    "all_adj = list(np.concatenate(all_adj))\n",
    "all_verbs = list(np.concatenate(all_verbs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'> Removing the boiler plate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_txt = ['project', 'broader', 'impact', 'Small', 'Business', 'Innovation', \n",
    "                '\\n', 'AI', 'SBIR','STTR', 'Phase', 'Research', 'merit', 'award', 'criterion',\n",
    "                'broad', 'intellectual', '-']\n",
    "\n",
    "for i in range(len(template_txt)):\n",
    "    remove_word = template_txt[i]\n",
    "    while remove_word in all_nouns:\n",
    "        all_nouns.remove(remove_word)\n",
    "    while remove_word in all_adj:\n",
    "        all_adj.remove(remove_word)\n",
    "    while remove_word in all_verbs:\n",
    "        all_verbs.remove(remove_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('datum', 101),\n",
       " ('model', 89),\n",
       " ('system', 81),\n",
       " ('learning', 81),\n",
       " ('support', 63),\n",
       " ('evaluation', 63),\n",
       " ('technology', 63),\n",
       " ('time', 59),\n",
       " ('mission', 59),\n",
       " ('review', 57),\n",
       " ('machine', 54),\n",
       " ('cost', 47),\n",
       " ('platform', 47),\n",
       " ('algorithm', 46),\n",
       " ('intelligence', 42),\n",
       " ('language', 42),\n",
       " ('student', 36),\n",
       " ('health', 35),\n",
       " ('research', 35),\n",
       " ('method', 34),\n",
       " ('application', 32),\n",
       " ('development', 32),\n",
       " ('information', 30),\n",
       " ('user', 29),\n",
       " ('training', 29),\n",
       " ('solution', 28),\n",
       " ('control', 26),\n",
       " ('video', 26),\n",
       " ('potential', 25),\n",
       " ('level', 25)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_list = Counter(all_nouns).most_common(30)\n",
    "noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('use', 141),\n",
       " ('develop', 64),\n",
       " ('reflect', 58),\n",
       " ('improve', 58),\n",
       " ('propose', 57),\n",
       " ('deem', 57),\n",
       " ('reduce', 55),\n",
       " ('provide', 54),\n",
       " ('learn', 49),\n",
       " ('base', 49),\n",
       " ('enable', 43),\n",
       " ('create', 40),\n",
       " ('increase', 40),\n",
       " ('have', 32),\n",
       " ('make', 31),\n",
       " ('include', 29),\n",
       " ('generate', 28),\n",
       " ('help', 28),\n",
       " ('require', 26),\n",
       " ('allow', 25),\n",
       " ('identify', 24),\n",
       " ('build', 23),\n",
       " ('result', 21),\n",
       " ('aim', 20),\n",
       " ('automate', 20),\n",
       " ('advance', 18),\n",
       " ('lead', 18),\n",
       " ('address', 18),\n",
       " ('train', 17),\n",
       " ('drive', 17)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_list = Counter(all_verbs).most_common(30)\n",
    "verb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('statutory', 57),\n",
       " ('worthy', 57),\n",
       " ('artificial', 41),\n",
       " ('new', 41),\n",
       " ('high', 36),\n",
       " ('commercial', 33),\n",
       " ('such', 29),\n",
       " ('real', 29),\n",
       " ('other', 27),\n",
       " ('current', 23),\n",
       " ('accurate', 23),\n",
       " ('human', 22),\n",
       " ('technical', 22),\n",
       " ('deep', 21),\n",
       " ('large', 18),\n",
       " ('novel', 17),\n",
       " ('advanced', 17),\n",
       " ('neural', 15),\n",
       " ('effective', 14),\n",
       " ('small', 14),\n",
       " ('low', 13),\n",
       " ('multiple', 13),\n",
       " ('many', 13),\n",
       " ('medical', 12),\n",
       " ('different', 12),\n",
       " ('social', 12),\n",
       " ('robust', 12),\n",
       " ('available', 12),\n",
       " ('reliable', 12),\n",
       " ('critical', 12)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_list = Counter(all_adj).most_common(30)\n",
    "adj_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>3. Find a subject/object relationship through the dependency parser in any sentence.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The broader impact of this Small Business Innovation Research (SBIR) Phase I project will result from creating a unique identification system using artificial intelligence (AI)-based facial recognition for horses and other animals that require vaccinations for birth control and disease inoculation. Wild horses and other wildlife that require remote vaccinations need to be identified so that populations are not over/under vaccinated. The means to vaccinate either manually or using remote technology exists, but most current methods are expensive, inhumane, or inefficient and identification is limited to photographs, sketches, memory, or RFID microchips. Federal agencies currently spend well over one hundred million dollars to deal with the problem. The commercial opportunity for a facial recognition system along with remote vaccination in this country and abroad is substantial. Wildlife managers will be able to relieve unhealthy overcrowding and allow livestock and domestic animals to co-exist with other species. This Small Business Innovation Research (SBIR) Phase I project will remotely and autonomously control the explosion of the wild horse populations on the open range. There are approximately 90,000 wild horses on the open range, which is 60,000 over the allowed maximum level. This is the level which is safe for horses and other animals which share the land. The need to vaccinate horses with contraceptive birth control is necessary to control the population. This project will develop an artificial intelligence (AI) identification system for horses using facial recognition technology and couple this with remote automated vaccination at feeding stations to ensure wild horses are correctly vaccinated for birth control and inoculated against disease. A comprehensive identification system based on facial recognition has commercial and societal impact beyond vaccinations for wild horses, extending to other wild species such as deer, and also to livestock. This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## just picking a random abstract to pick a sentence from\n",
    "abstracts_processed[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This project will develop an artificial intelligence (AI) identification system for horses using facial recognition technology and couple this with remote automated vaccination at feeding stations to ensure wild horses are correctly vaccinated for birth control and inoculated against disease.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sent = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN: project\n",
      "=====\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'develop'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: system\n",
      "=====\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'develop'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: horses\n",
      "=====\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'for'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: technology\n",
      "=====\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'using'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: this\n",
      "=====\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'couple'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: vaccination\n",
      "=====\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'with'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: stations\n",
      "=====\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'feeding'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: horses\n",
      "=====\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'ensure'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: control\n",
      "=====\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'for'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: disease\n",
      "=====\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'against'\n",
      "token.dep_ = 'pobj'\n"
     ]
    }
   ],
   "source": [
    "for token in processed_sent:\n",
    "    if token.dep_ == \"nsubj\" or  token.dep_ == \"dobj\" or token.dep_ == \"pobj\":\n",
    "        print(\n",
    "        f\"\"\"\n",
    "TOKEN: {token.text}\n",
    "=====\n",
    "{token.tag_ = }\n",
    "{token.head.text = }\n",
    "{token.dep_ = }\"\"\"\n",
    "     )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>4. Show the most common Entities and their types.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ents = []\n",
    "all_labels = []\n",
    "for i in range(len(abstracts_processed)):\n",
    "    text = abstracts_processed[i]\n",
    "    for entity in text.ents:\n",
    "        all_ents.append(entity)  \n",
    "        all_labels.append(entity.label_)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ents_labs = tuple(zip(all_ents, all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((this Small Business Innovation Research, 'ORG'), 1),\n",
       " ((American, 'NORP'), 1),\n",
       " ((American, 'NORP'), 1),\n",
       " ((12%, 'PERCENT'), 1),\n",
       " ((1990, 'DATE'), 1),\n",
       " ((over 40%, 'PERCENT'), 1),\n",
       " ((today, 'DATE'), 1),\n",
       " (($260 billion, 'MONEY'), 1),\n",
       " ((2016, 'DATE'), 1),\n",
       " ((the Center for Disease Control, 'ORG'), 1),\n",
       " ((CDC, 'ORG'), 1),\n",
       " ((the National Institute for Health, 'ORG'), 1),\n",
       " ((NIH, 'ORG'), 1),\n",
       " ((70%, 'PERCENT'), 1),\n",
       " ((American, 'NORP'), 1),\n",
       " ((2014, 'DATE'), 1),\n",
       " ((2013, 'DATE'), 1),\n",
       " ((American, 'NORP'), 1),\n",
       " (($60 billion, 'MONEY'), 1),\n",
       " ((annually, 'DATE'), 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(all_ents_labs).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>5. Find Entites and their dependency</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deps = []\n",
    "\n",
    "for entity in (all_ents):\n",
    "    all_deps.append(entity.root.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ents_deps = tuple(zip(all_ents, all_deps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_ents_deps = list(set(all_ents_deps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'> In the list of tuples, the first value of each tuple is the entity and the second value is the dependency </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(This Small Business Innovation Research, Phase),\n",
       " (This Small Business Technology Transfer, Phase),\n",
       " (NSF, mission),\n",
       " (Foundation, merit),\n",
       " (Foundation, merit),\n",
       " (annually, B),\n",
       " (millions, through),\n",
       " (NSF, mission),\n",
       " (hours, within),\n",
       " (1, Phase),\n",
       " (this Small Business Innovation Research, Phase),\n",
       " (Foundation, merit),\n",
       " (NSF, mission),\n",
       " (this Small Business Innovation Research, Phase),\n",
       " (This Small Business Innovation Research, Research),\n",
       " (this Small Business Innovation Research, Phase),\n",
       " (Science, Technology, Engineering, in),\n",
       " (x000D, x000D),\n",
       " (Riemannian, metrics),\n",
       " (this Small Business Innovation Research, centers)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_ents_deps[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>6. Instead of finding the most similar noun chunks on this one, I am going to see what the similarity scores of the keywords generated in Question 2 are, when compared to the term \"Artificial Intelligence\". I will also find the most common noun chunks and do the same.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'>Finding most common noun chunks. </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The broader impact\n",
      "(SBIR\n",
      "the health\n",
      "welfare\n",
      "the American public\n",
      "Obesity\n",
      "American adults\n",
      "12%\n",
      "over 40%\n",
      "an estimated medical cost\n",
      "the Center\n",
      "Disease Control\n",
      "(CDC\n",
      "the National Institute\n",
      "Health\n",
      "NIH\n",
      "70%\n",
      "American adults\n",
      "American adults\n",
      "weight loss\n",
      "US News\n",
      "World Report\n",
      "A 2008 American Journal\n",
      "Preventive Medicine study\n",
      "those\n",
      "who\n",
      "daily food journals\n",
      "twice as much weight\n",
      "those\n",
      "who\n",
      "existing diet tracking methods\n",
      "long-term weight loss\n",
      "A personalized artificial intelligence (AI) chatbot\n",
      "food\n",
      "fun\n",
      "millions\n",
      "Americans\n",
      "who\n",
      "weight\n",
      "knowledge\n",
      "spoken dialogue systems._x000D\n",
      "x000D\n",
      "(SBIR\n",
      "knowledge\n",
      "the field\n",
      "spoken dialogue systems\n",
      "several ways\n",
      "the project\n",
      "a new research area\n",
      "AI and spoken dialogue systems\n",
      "nutrition\n",
      "conversational agents\n",
      "factual question answering\n",
      "tasks\n",
      "flight booking\n",
      "an opportunity\n",
      "big data\n",
      "relationships\n",
      "diet\n",
      "health\n",
      "this project\n",
      "a neural generative chatbot model\n",
      "memory\n",
      "the benefit\n",
      "personalized conversational interactions\n",
      "intelligent agents\n",
      "that\n",
      "the history\n",
      "conversations\n",
      "personal details\n",
      "the user\n",
      "chatbot responses\n",
      "more control\n",
      "the output\n",
      "the drawback\n",
      "the responses\n",
      "This work\n",
      "generative Transformers\n",
      "order\n",
      "more realistic, human-like responses\n",
      "knowledge graphs\n",
      "a novel method\n",
      "the conversation\n",
      "history\n",
      "each user\n",
      "personalized feedback\n",
      "this project\n",
      "the application\n",
      "causal inference\n",
      "medical diagnosis\n",
      "the new, challenging task\n",
      "which\n",
      "foods\n",
      "outcomes\n",
      "gut symptoms\n",
      "weight loss\n",
      "building._x000D\n",
      "x000D\n",
      "This award\n",
      "NSF's statutory mission\n",
      "support\n",
      "evaluation\n",
      "the Foundation's intellectual merit\n",
      "broader impacts\n",
      "review criteria\n"
     ]
    }
   ],
   "source": [
    " for noun_chunk in abstracts_processed[0].noun_chunks:\n",
    "        print(noun_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nounchunks = []\n",
    "\n",
    "for i in range(len(abstracts_processed)):\n",
    "    for noun_chunk in abstracts_processed[i].noun_chunks:\n",
    "        all_nounchunks.append(noun_chunk)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(The broader impact, 1),\n",
       " ((SBIR, 1),\n",
       " (the health, 1),\n",
       " (welfare, 1),\n",
       " (the American public, 1),\n",
       " (Obesity, 1),\n",
       " (American adults, 1),\n",
       " (12%, 1),\n",
       " (over 40%, 1),\n",
       " (an estimated medical cost, 1),\n",
       " (the Center, 1),\n",
       " (Disease Control, 1),\n",
       " ((CDC, 1),\n",
       " (the National Institute, 1),\n",
       " (Health, 1),\n",
       " (NIH, 1),\n",
       " (70%, 1),\n",
       " (American adults, 1),\n",
       " (American adults, 1),\n",
       " (weight loss, 1),\n",
       " (US News, 1),\n",
       " (World Report, 1),\n",
       " (A 2008 American Journal, 1),\n",
       " (Preventive Medicine study, 1),\n",
       " (those, 1),\n",
       " (who, 1),\n",
       " (daily food journals, 1),\n",
       " (twice as much weight, 1),\n",
       " (those, 1),\n",
       " (who, 1),\n",
       " (existing diet tracking methods, 1),\n",
       " (long-term weight loss, 1),\n",
       " (A personalized artificial intelligence (AI) chatbot, 1),\n",
       " (food, 1),\n",
       " (fun, 1),\n",
       " (millions, 1),\n",
       " (Americans, 1),\n",
       " (who, 1),\n",
       " (weight, 1),\n",
       " (knowledge, 1),\n",
       " (spoken dialogue systems._x000D, 1),\n",
       " (x000D, 1),\n",
       " ((SBIR, 1),\n",
       " (knowledge, 1),\n",
       " (the field, 1),\n",
       " (spoken dialogue systems, 1),\n",
       " (several ways, 1),\n",
       " (the project, 1),\n",
       " (a new research area, 1),\n",
       " (AI and spoken dialogue systems, 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nounchunk_list = Counter(all_nounchunks).most_common(50)\n",
    "nounchunk_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'>Pulling just the words, not the counts, from most common word lists in Q2 and noun chunk list </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_adj_list=[]\n",
    "new_noun_list=[]\n",
    "new_verb_list=[]\n",
    "new_chunk_list = []\n",
    "\n",
    "for i in range(len(noun_list)):\n",
    "    noun = noun_list[i][0]\n",
    "    new_noun_list.append(noun)\n",
    "    \n",
    "for j in range(len(adj_list)):\n",
    "    adj = adj_list[j][0]\n",
    "    new_adj_list.append(adj)\n",
    "    \n",
    "for k in range(len(verb_list)):\n",
    "    verb = verb_list[k][0]\n",
    "    new_verb_list.append(verb)\n",
    "\n",
    "for c in range(len(nounchunk_list)):\n",
    "    nchunk = nounchunk_list[c][0]\n",
    "    new_chunk_list.append(nchunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'>Running the word lists through the nlp</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_processed = []\n",
    "\n",
    "for i in range(len(new_adj_list)):\n",
    "    adj_processed.append(nlp(str(new_adj_list[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_processed = []\n",
    "\n",
    "for i in range(len(new_verb_list)):\n",
    "    verb_processed.append(nlp(str(new_verb_list[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_processed = []\n",
    "\n",
    "for i in range(len(new_noun_list)):\n",
    "    noun_processed.append(nlp(str(new_noun_list[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_processed = []\n",
    "\n",
    "for i in range(len(new_chunk_list)):\n",
    "    chunk_processed.append(nlp(str(new_chunk_list[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='purple'>Now let's see how similar each of these tokens are to \"Artificial Intelligence\"</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sim_scores = []\n",
    "for i in range(len(noun_processed)):\n",
    "    score_tup = (noun_processed[i], noun_processed[i].similarity(nlp(\"Artificial Intelligence\")))\n",
    "    noun_sim_scores.append(score_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_sim_scores = []\n",
    "for i in range(len(adj_processed)):\n",
    "    score_tup = (adj_processed[i], adj_processed[i].similarity(nlp(\"Artificial Intelligence\")))\n",
    "    adj_sim_scores.append(score_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_sim_scores = []\n",
    "for i in range(len(verb_processed)):\n",
    "    score_tup = (verb_processed[i], verb_processed[i].similarity(nlp(\"Artificial Intelligence\")))\n",
    "    verb_sim_scores.append(score_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hf/hkt15zzd0t90_vcw4b8l07mr0000gn/T/ipykernel_89181/2537215597.py:3: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  score_tup = (chunk_processed[i], chunk_processed[i].similarity(nlp(\"Artificial Intelligence\")))\n"
     ]
    }
   ],
   "source": [
    "chunk_sim_scores = []\n",
    "for i in range(len(chunk_processed)):\n",
    "    score_tup = (chunk_processed[i], chunk_processed[i].similarity(nlp(\"Artificial Intelligence\")))\n",
    "    chunk_sim_scores.append(score_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datum, 0.16229846542910573),\n",
       " (model, 0.3584834386589548),\n",
       " (system, 0.4184719545872337),\n",
       " (learning, 0.46545453761360495),\n",
       " (support, 0.45484264365717275),\n",
       " (evaluation, 0.611053014799221),\n",
       " (technology, 0.5920905624578098),\n",
       " (time, 0.1551375607866306),\n",
       " (mission, 0.49339629497424203),\n",
       " (review, 0.368899479603463),\n",
       " (machine, 0.29071500562829494),\n",
       " (cost, 0.13468692324543607),\n",
       " (platform, 0.44343556096788234),\n",
       " (algorithm, 0.37940958735720265),\n",
       " (intelligence, 0.7534147843071304),\n",
       " (language, 0.3600757617059399),\n",
       " (student, 0.29192001859595706),\n",
       " (health, 0.30730281271486815),\n",
       " (research, 0.5477582517676998),\n",
       " (method, 0.3518276609476514),\n",
       " (application, 0.6059586760265948),\n",
       " (development, 0.5687852237378012),\n",
       " (information, 0.5713244979079949),\n",
       " (user, 0.24889266108900693),\n",
       " (training, 0.4700695505090661),\n",
       " (solution, 0.516126339600525),\n",
       " (control, 0.4422738424290547),\n",
       " (video, 0.2075852411612967),\n",
       " (potential, 0.4586656792996185),\n",
       " (level, 0.340708301894093)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(use, 0.2708556199461809),\n",
       " (develop, 0.4382611501030176),\n",
       " (reflect, 0.23552155365401065),\n",
       " (improve, 0.3367729849605888),\n",
       " (propose, 0.36343975973940656),\n",
       " (deem, 0.14918708598538288),\n",
       " (reduce, 0.24169892192303288),\n",
       " (provide, 0.37870110973974336),\n",
       " (learn, 0.15861211260498925),\n",
       " (base, 0.28112849797286527),\n",
       " (enable, 0.4342192174281438),\n",
       " (create, 0.32369977944425465),\n",
       " (increase, 0.3470589400107229),\n",
       " (have, 0.16567279131147744),\n",
       " (make, 0.07299417311196903),\n",
       " (include, 0.3941786506152489),\n",
       " (generate, 0.3789617673713044),\n",
       " (help, 0.19288026401161093),\n",
       " (require, 0.3038872149764361),\n",
       " (allow, 0.23903739008843508),\n",
       " (identify, 0.419122005553052),\n",
       " (build, 0.21127415143505704),\n",
       " (result, 0.3396724185598072),\n",
       " (aim, 0.19195868838080615),\n",
       " (automate, 0.4925967332225406),\n",
       " (advance, 0.4530784082293404),\n",
       " (lead, 0.20234254075420816),\n",
       " (address, 0.3282341942434057),\n",
       " (train, 0.11416316395336243),\n",
       " (drive, 0.07277728018849687)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(statutory, 0.34161351467311185),\n",
       " (worthy, 0.02409471807301838),\n",
       " (artificial, 0.7181813251221405),\n",
       " (new, 0.2594265977444017),\n",
       " (high, 0.21745876260938274),\n",
       " (commercial, 0.44930071010520195),\n",
       " (such, 0.33926811868514983),\n",
       " (real, 0.1976167858195671),\n",
       " (other, 0.37166003257766117),\n",
       " (current, 0.4403509108482855),\n",
       " (accurate, 0.3580312914939001),\n",
       " (human, 0.38616404703687013),\n",
       " (technical, 0.5654320535617686),\n",
       " (deep, 0.14236245576745077),\n",
       " (large, 0.27610697255206146),\n",
       " (novel, 0.14623246434457848),\n",
       " (advanced, 0.5232243321366429),\n",
       " (neural, 0.41318993503194257),\n",
       " (effective, 0.5181157915169209),\n",
       " (small, 0.19171740224475403),\n",
       " (low, 0.10188361339250633),\n",
       " (multiple, 0.37848785364242427),\n",
       " (many, 0.21052204542830283),\n",
       " (medical, 0.3901301222397854),\n",
       " (different, 0.37415293934846),\n",
       " (social, 0.3637190322616282),\n",
       " (robust, 0.29570670759182394),\n",
       " (available, 0.40672720986020416),\n",
       " (reliable, 0.42674694652927575),\n",
       " (critical, 0.5284022278859833)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(The broader impact, 0.49764284122726915),\n",
       " ((SBIR, 0.2450353185621908),\n",
       " (the health, 0.4669891702642012),\n",
       " (welfare, 0.21816397763488432),\n",
       " (the American public, 0.518324983808169),\n",
       " (Obesity, 0.2323724093992703),\n",
       " (American adults, 0.38860581864549093),\n",
       " (12%, 0.04107140033942633),\n",
       " (over 40%, 0.06492811747024285),\n",
       " (an estimated medical cost, 0.3351983507705724),\n",
       " (the Center, 0.5314765390451474),\n",
       " (Disease Control, 0.5481757777291704),\n",
       " ((CDC, 0.22819281941103012),\n",
       " (the National Institute, 0.6091128569315473),\n",
       " (Health, 0.4106357266719943),\n",
       " (NIH, 0.18224507944486326),\n",
       " (70%, 0.04232412947204514),\n",
       " (American adults, 0.38860581864549093),\n",
       " (American adults, 0.38860581864549093),\n",
       " (weight loss, 0.1809382545871975),\n",
       " (US News, 0.36099220663633097),\n",
       " (World Report, 0.5187706762296186),\n",
       " (A 2008 American Journal, 0.3932253594670052),\n",
       " (Preventive Medicine study, 0.5865217631512722),\n",
       " (those, 0.20376603473029403),\n",
       " (who, 0.078623235718873),\n",
       " (daily food journals, 0.2600020837104913),\n",
       " (twice as much weight, 0.24234356527557854),\n",
       " (those, 0.20376603473029403),\n",
       " (who, 0.078623235718873),\n",
       " (existing diet tracking methods, 0.4821760890865779),\n",
       " (long-term weight loss, 0.29153342104732305),\n",
       " (A personalized artificial intelligence (AI) chatbot, 0.5730297946412343),\n",
       " (food, 0.11188620931842633),\n",
       " (fun, -0.05856397079918373),\n",
       " (millions, 0.27947875913885306),\n",
       " (Americans, 0.3055541246483523),\n",
       " (who, 0.078623235718873),\n",
       " (weight, 0.20691235855845205),\n",
       " (knowledge, 0.4911135228538597),\n",
       " (spoken dialogue systems._x000D, 0.21507786363940673),\n",
       " (x000D, 0.0),\n",
       " ((SBIR, 0.2450353185621908),\n",
       " (knowledge, 0.4911135228538597),\n",
       " (the field, 0.4820226949924722),\n",
       " (spoken dialogue systems, 0.4724625370548312),\n",
       " (several ways, 0.3579166482605704),\n",
       " (the project, 0.5159254115803398),\n",
       " (a new research area, 0.3777704942436021),\n",
       " (AI and spoken dialogue systems, 0.6579521391424739)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_sim_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='purple'>Here are the highest similarity scores </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(artificial, 0.7181813251221405)\n",
      "(commercial, 0.44930071010520195)\n",
      "(current, 0.4403509108482855)\n",
      "(technical, 0.5654320535617686)\n",
      "(advanced, 0.5232243321366429)\n",
      "(neural, 0.41318993503194257)\n",
      "(effective, 0.5181157915169209)\n",
      "(available, 0.40672720986020416)\n",
      "(reliable, 0.42674694652927575)\n",
      "(critical, 0.5284022278859833)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(adj_sim_scores)):\n",
    "    if adj_sim_scores[i][1] > 0.40:\n",
    "        print(adj_sim_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(develop, 0.4382611501030176)\n",
      "(enable, 0.4342192174281438)\n",
      "(identify, 0.419122005553052)\n",
      "(automate, 0.4925967332225406)\n",
      "(advance, 0.4530784082293404)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(verb_sim_scores)):\n",
    "    if verb_sim_scores[i][1] > 0.4:\n",
    "        print(verb_sim_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(evaluation, 0.611053014799221)\n",
      "(technology, 0.5920905624578098)\n",
      "(intelligence, 0.7534147843071304)\n",
      "(research, 0.5477582517676998)\n",
      "(application, 0.6059586760265948)\n",
      "(development, 0.5687852237378012)\n",
      "(information, 0.5713244979079949)\n",
      "(solution, 0.516126339600525)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(noun_sim_scores)):\n",
    "    if noun_sim_scores[i][1] > 0.50:\n",
    "        print(noun_sim_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(The broader impact, 0.49764284122726915)\n",
      "(the health, 0.4669891702642012)\n",
      "(the American public, 0.518324983808169)\n",
      "(the Center, 0.5314765390451474)\n",
      "(Disease Control, 0.5481757777291704)\n",
      "(the National Institute, 0.6091128569315473)\n",
      "(Health, 0.4106357266719943)\n",
      "(World Report, 0.5187706762296186)\n",
      "(Preventive Medicine study, 0.5865217631512722)\n",
      "(existing diet tracking methods, 0.4821760890865779)\n",
      "(A personalized artificial intelligence (AI) chatbot, 0.5730297946412343)\n",
      "(knowledge, 0.4911135228538597)\n",
      "(knowledge, 0.4911135228538597)\n",
      "(the field, 0.4820226949924722)\n",
      "(spoken dialogue systems, 0.4724625370548312)\n",
      "(the project, 0.5159254115803398)\n",
      "(AI and spoken dialogue systems, 0.6579521391424739)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(chunk_sim_scores)):\n",
    "    if chunk_sim_scores[i][1] > 0.4:\n",
    "        print(chunk_sim_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
